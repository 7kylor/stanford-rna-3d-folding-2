{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stanford RNA 3D Folding Part 2 - Complete Pipeline\n",
                "\n",
                "**Template-Based Modeling (TBM) with Enhanced Deep Learning Features**\n",
                "\n",
                "This comprehensive notebook implements the full RNA 3D structure prediction pipeline:\n",
                "\n",
                "## Pipeline Overview\n",
                "1. **Template Search** - K-mer indexed PDB search with temporal filtering\n",
                "2. **Sequence Alignment** - Needleman-Wunsch global alignment\n",
                "3. **Coordinate Transfer** - Map C1' atoms from template to target\n",
                "4. **Gap Filling** - Geometric interpolation for unmapped residues\n",
                "5. **RNA-FM Embeddings** - Deep learning enhanced similarity search\n",
                "6. **Model Diversity** - Generate 5 diverse predictions per target\n",
                "7. **Validation** - TM-score computation against ground truth\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Environment: Local\n",
                        "Data directory: /Users/taher/Projects/stanford-rna-3d-folding-2\n",
                        "Output directory: /Users/taher/Projects/stanford-rna-3d-folding-2/output\n"
                    ]
                }
            ],
            "source": [
                "import csv\n",
                "import os\n",
                "import pickle\n",
                "import re\n",
                "import sys\n",
                "import time\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "from dataclasses import dataclass\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, NamedTuple, Optional, Tuple\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.interpolate import interp1d\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Detect environment\n",
                "IS_KAGGLE = os.path.exists(\"/kaggle\")\n",
                "\n",
                "if IS_KAGGLE:\n",
                "    DATA_DIR = Path(\"/kaggle/input/stanford-rna-3d-folding-2\")\n",
                "    OUTPUT_DIR = Path(\"/kaggle/working\")\n",
                "else:\n",
                "    DATA_DIR = Path(\"..\").resolve()\n",
                "    OUTPUT_DIR = Path(\"../output\").resolve()\n",
                "\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
                "print(f\"Data directory: {DATA_DIR}\")\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Configuration: PipelineConfig(kmer_size=6, max_template_hits=50, min_identity=0.25, min_coverage=0.25, num_models=5, perturbation_scale=0.2, average_c1_distance=5.9, use_embeddings=True, max_files=None)\n"
                    ]
                }
            ],
            "source": [
                "# Pipeline configuration\n",
                "@dataclass\n",
                "class PipelineConfig:\n",
                "    \"\"\"Configuration for the TBM pipeline.\"\"\"\n",
                "    kmer_size: int = 6\n",
                "    max_template_hits: int = 50\n",
                "    min_identity: float = 0.25\n",
                "    min_coverage: float = 0.25\n",
                "    num_models: int = 5\n",
                "    perturbation_scale: float = 0.2\n",
                "    average_c1_distance: float = 5.9  # Angstroms\n",
                "    use_embeddings: bool = True\n",
                "    max_files: int = None  # Limit for testing\n",
                "\n",
                "config = PipelineConfig()\n",
                "print(f\"Configuration: {config}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Core Data Structures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 40 modified base mappings\n"
                    ]
                }
            ],
            "source": [
                "class Residue(NamedTuple):\n",
                "    \"\"\"Single residue with C1' coordinates.\"\"\"\n",
                "    resname: str\n",
                "    resid: int\n",
                "    chain: str\n",
                "    x: float\n",
                "    y: float\n",
                "    z: float\n",
                "\n",
                "class ChainCoords(NamedTuple):\n",
                "    \"\"\"All C1' coordinates for a single chain.\"\"\"\n",
                "    chain_id: str\n",
                "    sequence: str\n",
                "    residues: List[Residue]\n",
                "\n",
                "class TemplateHit(NamedTuple):\n",
                "    \"\"\"A template match result.\"\"\"\n",
                "    pdb_id: str\n",
                "    chain_id: str\n",
                "    sequence: str\n",
                "    release_date: str\n",
                "    identity: float\n",
                "    coverage: float\n",
                "\n",
                "# Modified nucleotide mappings\n",
                "MODIFIED_BASE_MAP = {\n",
                "    'A': 'A', 'ADE': 'A', 'DA': 'A', '1MA': 'A', '2MA': 'A', 'MIA': 'A',\n",
                "    'T6A': 'A', 'I': 'A', 'INO': 'A', '6MA': 'A', 'A2M': 'A', 'MA6': 'A',\n",
                "    'C': 'C', 'CYT': 'C', 'DC': 'C', '5MC': 'C', 'OMC': 'C', '4OC': 'C',\n",
                "    'G': 'G', 'GUA': 'G', 'DG': 'G', '1MG': 'G', '2MG': 'G', '7MG': 'G',\n",
                "    'M2G': 'G', 'OMG': 'G', 'YG': 'G',\n",
                "    'U': 'U', 'URA': 'U', 'DU': 'U', 'PSU': 'U', '5MU': 'U', 'H2U': 'U',\n",
                "    'OMU': 'U', 'S4U': 'U', '4SU': 'U', 'DHU': 'U',\n",
                "    'T': 'U', 'DT': 'U', 'THY': 'U',\n",
                "}\n",
                "print(f\"Loaded {len(MODIFIED_BASE_MAP)} modified base mappings\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. CIF Parser"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CIF Parser loaded\n"
                    ]
                }
            ],
            "source": [
                "def parse_cif_line(line: str) -> List[str]:\n",
                "    \"\"\"Parse a CIF data line, handling quoted strings.\"\"\"\n",
                "    tokens = []\n",
                "    i = 0\n",
                "    while i < len(line):\n",
                "        while i < len(line) and line[i] in ' \\t':\n",
                "            i += 1\n",
                "        if i >= len(line):\n",
                "            break\n",
                "        if line[i] in '\"\\'':\n",
                "            quote = line[i]\n",
                "            i += 1\n",
                "            start = i\n",
                "            while i < len(line) and line[i] != quote:\n",
                "                i += 1\n",
                "            tokens.append(line[start:i])\n",
                "            i += 1\n",
                "        else:\n",
                "            start = i\n",
                "            while i < len(line) and line[i] not in ' \\t':\n",
                "                i += 1\n",
                "            tokens.append(line[start:i])\n",
                "    return tokens\n",
                "\n",
                "\n",
                "def parse_cif_c1prime(cif_path: str) -> Dict[str, ChainCoords]:\n",
                "    \"\"\"Parse mmCIF file and extract C1' coordinates for RNA chains.\"\"\"\n",
                "    with open(cif_path, 'r') as f:\n",
                "        content = f.read()\n",
                "    \n",
                "    atom_site_match = re.search(r'loop_\\s*\\n(_atom_site\\.\\w+\\s*\\n)+', content)\n",
                "    if not atom_site_match:\n",
                "        return {}\n",
                "    \n",
                "    header_section = atom_site_match.group(0)\n",
                "    columns = re.findall(r'_atom_site\\.(\\w+)', header_section)\n",
                "    \n",
                "    try:\n",
                "        col_indices = {\n",
                "            'group_PDB': columns.index('group_PDB'),\n",
                "            'label_atom_id': columns.index('label_atom_id'),\n",
                "            'label_comp_id': columns.index('label_comp_id'),\n",
                "            'auth_asym_id': columns.index('auth_asym_id'),\n",
                "            'auth_seq_id': columns.index('auth_seq_id'),\n",
                "            'Cartn_x': columns.index('Cartn_x'),\n",
                "            'Cartn_y': columns.index('Cartn_y'),\n",
                "            'Cartn_z': columns.index('Cartn_z'),\n",
                "        }\n",
                "        if 'pdbx_PDB_model_num' in columns:\n",
                "            col_indices['pdbx_PDB_model_num'] = columns.index('pdbx_PDB_model_num')\n",
                "        else:\n",
                "            col_indices['pdbx_PDB_model_num'] = None\n",
                "    except ValueError:\n",
                "        return {}\n",
                "    \n",
                "    data_start = atom_site_match.end()\n",
                "    lines = content[data_start:].split('\\n')\n",
                "    \n",
                "    chain_residues: Dict[str, Dict[int, Residue]] = {}\n",
                "    \n",
                "    for line in lines:\n",
                "        line = line.strip()\n",
                "        if not line or line.startswith('_') or line.startswith('#') or line.startswith('loop_'):\n",
                "            if line.startswith('#') or line.startswith('loop_'):\n",
                "                break\n",
                "            continue\n",
                "        \n",
                "        tokens = parse_cif_line(line)\n",
                "        if len(tokens) < max(v for v in col_indices.values() if v is not None) + 1:\n",
                "            continue\n",
                "        \n",
                "        if col_indices['pdbx_PDB_model_num'] is not None:\n",
                "            model_num = tokens[col_indices['pdbx_PDB_model_num']]\n",
                "            if model_num != '1' and model_num != '?':\n",
                "                continue\n",
                "        \n",
                "        group = tokens[col_indices['group_PDB']]\n",
                "        atom_id = tokens[col_indices['label_atom_id']]\n",
                "        if len(atom_id) >= 2 and atom_id[0] == atom_id[-1] and atom_id[0] in '\"\\'':\n",
                "            atom_id = atom_id[1:-1]\n",
                "        \n",
                "        if group != 'ATOM' or atom_id != \"C1'\":\n",
                "            continue\n",
                "        \n",
                "        comp_id = tokens[col_indices['label_comp_id']]\n",
                "        base = MODIFIED_BASE_MAP.get(comp_id.upper(), comp_id.upper())\n",
                "        if base not in {'A', 'C', 'G', 'U'}:\n",
                "            continue\n",
                "        \n",
                "        chain = tokens[col_indices['auth_asym_id']]\n",
                "        try:\n",
                "            resid = int(tokens[col_indices['auth_seq_id']])\n",
                "            x = float(tokens[col_indices['Cartn_x']])\n",
                "            y = float(tokens[col_indices['Cartn_y']])\n",
                "            z = float(tokens[col_indices['Cartn_z']])\n",
                "        except (ValueError, IndexError):\n",
                "            continue\n",
                "        \n",
                "        if chain not in chain_residues:\n",
                "            chain_residues[chain] = {}\n",
                "        \n",
                "        if resid not in chain_residues[chain]:\n",
                "            chain_residues[chain][resid] = Residue(base, resid, chain, x, y, z)\n",
                "    \n",
                "    result = {}\n",
                "    for chain_id, res_dict in chain_residues.items():\n",
                "        sorted_resids = sorted(res_dict.keys())\n",
                "        residues = [res_dict[rid] for rid in sorted_resids]\n",
                "        sequence = ''.join(r.resname for r in residues)\n",
                "        result[chain_id] = ChainCoords(chain_id, sequence, residues)\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"CIF Parser loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Template Database with K-mer Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TemplateDB class loaded\n"
                    ]
                }
            ],
            "source": [
                "class TemplateDB:\n",
                "    \"\"\"Template database with k-mer index for fast sequence search.\"\"\"\n",
                "    \n",
                "    def __init__(self, k: int = 6):\n",
                "        self.k = k\n",
                "        self.templates: Dict[str, Dict[str, ChainCoords]] = {}\n",
                "        self.release_dates: Dict[str, str] = {}\n",
                "        self.kmer_index: Dict[str, List[Tuple[str, str, int]]] = defaultdict(list)\n",
                "        self.sequences: Dict[Tuple[str, str], str] = {}\n",
                "    \n",
                "    def build_from_directory(self, cif_dir: str, release_dates_file: str = None, max_files: int = None):\n",
                "        \"\"\"Build template database from PDB_RNA directory.\"\"\"\n",
                "        if release_dates_file and os.path.exists(release_dates_file):\n",
                "            self._load_release_dates(release_dates_file)\n",
                "        \n",
                "        cif_files = list(Path(cif_dir).glob('*.cif'))\n",
                "        if max_files:\n",
                "            cif_files = cif_files[:max_files]\n",
                "        total = len(cif_files)\n",
                "        print(f\"Building template database from {total} CIF files...\")\n",
                "        \n",
                "        for i, cif_path in enumerate(cif_files):\n",
                "            pdb_id = cif_path.stem.upper()\n",
                "            \n",
                "            if release_dates_file and pdb_id not in self.release_dates:\n",
                "                continue\n",
                "            \n",
                "            try:\n",
                "                chains = parse_cif_c1prime(str(cif_path))\n",
                "                if chains:\n",
                "                    self.templates[pdb_id] = chains\n",
                "                    for chain_id, chain_coords in chains.items():\n",
                "                        seq = chain_coords.sequence\n",
                "                        self.sequences[(pdb_id, chain_id)] = seq\n",
                "                        for pos in range(len(seq) - self.k + 1):\n",
                "                            kmer = seq[pos:pos + self.k]\n",
                "                            self.kmer_index[kmer].append((pdb_id, chain_id, pos))\n",
                "            except Exception:\n",
                "                pass\n",
                "            \n",
                "            if (i + 1) % 500 == 0:\n",
                "                print(f\"  Processed {i+1}/{total} files...\")\n",
                "        \n",
                "        print(f\"Done. {len(self.templates)} templates indexed.\")\n",
                "    \n",
                "    def _load_release_dates(self, csv_path: str):\n",
                "        \"\"\"Load release dates from CSV.\"\"\"\n",
                "        count = 0\n",
                "        with open(csv_path, 'r') as f:\n",
                "            reader = csv.DictReader(f)\n",
                "            for row in reader:\n",
                "                pdb_id = row.get('pdb_id', row.get('entry_id', '')).upper()\n",
                "                if not pdb_id and 'target_id' in row:\n",
                "                    target_id = row['target_id']\n",
                "                    pdb_id = target_id.split('_')[0].upper() if '_' in target_id else target_id[:4].upper()\n",
                "                \n",
                "                date = row.get('release_date', row.get('Release Date', row.get('temporal_cutoff', '')))\n",
                "                if pdb_id and date and pdb_id not in self.release_dates:\n",
                "                    self.release_dates[pdb_id] = date\n",
                "                    count += 1\n",
                "        print(f\"  Loaded {count} release dates\")\n",
                "    \n",
                "    def search(self, query_sequence: str, temporal_cutoff: str, max_hits: int = 50, min_identity: float = 0.25) -> List[TemplateHit]:\n",
                "        \"\"\"Search for template matches.\"\"\"\n",
                "        cutoff_date = datetime.strptime(temporal_cutoff, '%Y-%m-%d')\n",
                "        hit_counts: Dict[Tuple[str, str], int] = defaultdict(int)\n",
                "        \n",
                "        for pos in range(len(query_sequence) - self.k + 1):\n",
                "            kmer = query_sequence[pos:pos + self.k]\n",
                "            if kmer in self.kmer_index:\n",
                "                for pdb_id, chain_id, _ in self.kmer_index[kmer]:\n",
                "                    release_date_str = self.release_dates.get(pdb_id, '9999-12-31')\n",
                "                    try:\n",
                "                        release_date = datetime.strptime(release_date_str, '%Y-%m-%d')\n",
                "                        if release_date >= cutoff_date:\n",
                "                            continue\n",
                "                    except:\n",
                "                        continue\n",
                "                    hit_counts[(pdb_id, chain_id)] += 1\n",
                "        \n",
                "        candidates = sorted(hit_counts.items(), key=lambda x: -x[1])[:max_hits * 2]\n",
                "        \n",
                "        results = []\n",
                "        for (pdb_id, chain_id), count in candidates:\n",
                "            template_seq = self.sequences[(pdb_id, chain_id)]\n",
                "            identity, coverage = self._quick_align(query_sequence, template_seq)\n",
                "            \n",
                "            if identity >= min_identity:\n",
                "                results.append(TemplateHit(\n",
                "                    pdb_id=pdb_id, chain_id=chain_id, sequence=template_seq,\n",
                "                    release_date=self.release_dates.get(pdb_id, 'unknown'),\n",
                "                    identity=identity, coverage=coverage\n",
                "                ))\n",
                "        \n",
                "        results.sort(key=lambda h: -(h.identity * h.coverage))\n",
                "        return results[:max_hits]\n",
                "    \n",
                "    def _quick_align(self, query: str, template: str) -> Tuple[float, float]:\n",
                "        \"\"\"Quick diagonal alignment for scoring.\"\"\"\n",
                "        seeds = []\n",
                "        for q_pos in range(len(query) - self.k + 1):\n",
                "            kmer = query[q_pos:q_pos + self.k]\n",
                "            for t_pos in range(len(template) - self.k + 1):\n",
                "                if template[t_pos:t_pos + self.k] == kmer:\n",
                "                    seeds.append((q_pos, t_pos))\n",
                "        \n",
                "        if not seeds:\n",
                "            return 0.0, 0.0\n",
                "        \n",
                "        diag_seeds: Dict[int, int] = defaultdict(int)\n",
                "        for q_pos, t_pos in seeds:\n",
                "            diag_seeds[q_pos - t_pos] += 1\n",
                "        \n",
                "        best_diag = max(diag_seeds.keys(), key=lambda d: diag_seeds[d])\n",
                "        offset = -best_diag\n",
                "        \n",
                "        matches = 0\n",
                "        aligned = 0\n",
                "        for q_idx in range(len(query)):\n",
                "            t_idx = q_idx + offset\n",
                "            if 0 <= t_idx < len(template):\n",
                "                aligned += 1\n",
                "                if query[q_idx] == template[t_idx]:\n",
                "                    matches += 1\n",
                "        \n",
                "        identity = matches / len(query) if query else 0\n",
                "        coverage = aligned / len(query) if query else 0\n",
                "        return identity, coverage\n",
                "    \n",
                "    def get_template_coords(self, pdb_id: str, chain_id: str) -> Optional[ChainCoords]:\n",
                "        \"\"\"Get coordinates for a specific template.\"\"\"\n",
                "        if pdb_id in self.templates and chain_id in self.templates[pdb_id]:\n",
                "            return self.templates[pdb_id][chain_id]\n",
                "        return None\n",
                "    \n",
                "    def save(self, path: str):\n",
                "        \"\"\"Save database to pickle file.\"\"\"\n",
                "        data = {\n",
                "            'k': self.k, 'templates': self.templates,\n",
                "            'release_dates': self.release_dates,\n",
                "            'kmer_index': dict(self.kmer_index),\n",
                "            'sequences': self.sequences,\n",
                "        }\n",
                "        with open(path, 'wb') as f:\n",
                "            pickle.dump(data, f)\n",
                "    \n",
                "    @classmethod\n",
                "    def load(cls, path: str) -> 'TemplateDB':\n",
                "        \"\"\"Load database from pickle file.\"\"\"\n",
                "        with open(path, 'rb') as f:\n",
                "            data = pickle.load(f)\n",
                "        db = cls(k=data['k'])\n",
                "        db.templates = data['templates']\n",
                "        db.release_dates = data['release_dates']\n",
                "        db.kmer_index = defaultdict(list, data['kmer_index'])\n",
                "        db.sequences = data['sequences']\n",
                "        return db\n",
                "\n",
                "print(\"TemplateDB class loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sequence Alignment (Needleman-Wunsch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Alignment functions loaded\n"
                    ]
                }
            ],
            "source": [
                "def needleman_wunsch(seq1: str, seq2: str) -> List[Tuple[int, int]]:\n",
                "    \"\"\"Global alignment returning aligned pairs.\"\"\"\n",
                "    n, m = len(seq1), len(seq2)\n",
                "    \n",
                "    dp = np.zeros((n + 1, m + 1), dtype=np.int32)\n",
                "    for i in range(n + 1):\n",
                "        dp[i, 0] = i * -2\n",
                "    for j in range(m + 1):\n",
                "        dp[0, j] = j * -2\n",
                "    \n",
                "    for i in range(1, n + 1):\n",
                "        for j in range(1, m + 1):\n",
                "            match = dp[i-1, j-1] + (2 if seq1[i-1] == seq2[j-1] else -1)\n",
                "            delete = dp[i-1, j] - 2\n",
                "            insert = dp[i, j-1] - 2\n",
                "            dp[i, j] = max(match, delete, insert)\n",
                "    \n",
                "    aligned_pairs = []\n",
                "    i, j = n, m\n",
                "    \n",
                "    while i > 0 or j > 0:\n",
                "        if i > 0 and j > 0:\n",
                "            score = 2 if seq1[i-1] == seq2[j-1] else -1\n",
                "            if dp[i, j] == dp[i-1, j-1] + score:\n",
                "                aligned_pairs.append((i-1, j-1))\n",
                "                i -= 1\n",
                "                j -= 1\n",
                "                continue\n",
                "        if i > 0 and dp[i, j] == dp[i-1, j] - 2:\n",
                "            i -= 1\n",
                "        else:\n",
                "            j -= 1\n",
                "    \n",
                "    aligned_pairs.reverse()\n",
                "    return aligned_pairs\n",
                "\n",
                "\n",
                "def transfer_coordinates(query_seq: str, template_coords: ChainCoords) -> np.ndarray:\n",
                "    \"\"\"Transfer C1' coordinates from template to query.\"\"\"\n",
                "    aligned_pairs = needleman_wunsch(query_seq, template_coords.sequence)\n",
                "    \n",
                "    n = len(query_seq)\n",
                "    coords = np.full((n, 3), np.nan, dtype=np.float64)\n",
                "    \n",
                "    template_lookup = {\n",
                "        i: (r.x, r.y, r.z) for i, r in enumerate(template_coords.residues)\n",
                "    }\n",
                "    \n",
                "    for q_idx, t_idx in aligned_pairs:\n",
                "        if t_idx in template_lookup:\n",
                "            coords[q_idx] = template_lookup[t_idx]\n",
                "    \n",
                "    return coords\n",
                "\n",
                "print(\"Alignment functions loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Gap Filling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gap filling functions loaded\n"
                    ]
                }
            ],
            "source": [
                "def generate_geometric_baseline(n: int, rise_per_residue: float = 2.8, radius: float = 10.0) -> np.ndarray:\n",
                "    \"\"\"Generate an A-form helix baseline structure.\"\"\"\n",
                "    coords = np.zeros((n, 3))\n",
                "    residues_per_turn = 11\n",
                "    \n",
                "    for i in range(n):\n",
                "        angle = 2 * np.pi * i / residues_per_turn\n",
                "        coords[i, 0] = radius * np.cos(angle)\n",
                "        coords[i, 1] = radius * np.sin(angle)\n",
                "        coords[i, 2] = i * rise_per_residue\n",
                "    \n",
                "    return coords\n",
                "\n",
                "\n",
                "def fill_gaps(coords: np.ndarray, avg_distance: float = 5.9) -> np.ndarray:\n",
                "    \"\"\"Fill gaps using geometric interpolation.\"\"\"\n",
                "    n = len(coords)\n",
                "    if n == 0:\n",
                "        return coords\n",
                "    \n",
                "    filled = coords.copy()\n",
                "    mapped_indices = [i for i in range(n) if not np.isnan(coords[i, 0])]\n",
                "    \n",
                "    if len(mapped_indices) == 0:\n",
                "        return generate_geometric_baseline(n)\n",
                "    \n",
                "    if len(mapped_indices) == 1:\n",
                "        anchor_idx = mapped_indices[0]\n",
                "        direction = np.array([avg_distance, 0, 0])\n",
                "        for i in range(anchor_idx - 1, -1, -1):\n",
                "            filled[i] = filled[i + 1] - direction\n",
                "        for i in range(anchor_idx + 1, n):\n",
                "            filled[i] = filled[i - 1] + direction\n",
                "        return filled\n",
                "    \n",
                "    sorted_anchors = sorted(mapped_indices)\n",
                "    \n",
                "    # Fill internal gaps\n",
                "    for i in range(len(sorted_anchors) - 1):\n",
                "        start_idx = sorted_anchors[i]\n",
                "        end_idx = sorted_anchors[i + 1]\n",
                "        if end_idx - start_idx > 1:\n",
                "            start_coord = coords[start_idx]\n",
                "            end_coord = coords[end_idx]\n",
                "            gap_length = end_idx - start_idx\n",
                "            for j in range(1, gap_length):\n",
                "                t = j / gap_length\n",
                "                filled[start_idx + j] = start_coord + t * (end_coord - start_coord)\n",
                "    \n",
                "    # Fill leading gap\n",
                "    if sorted_anchors[0] > 0:\n",
                "        first_anchor = sorted_anchors[0]\n",
                "        if len(sorted_anchors) >= 2:\n",
                "            direction = coords[first_anchor] - coords[sorted_anchors[1]]\n",
                "            norm = np.linalg.norm(direction)\n",
                "            direction = direction / norm * avg_distance if norm > 0 else np.array([-avg_distance, 0, 0])\n",
                "        else:\n",
                "            direction = np.array([-avg_distance, 0, 0])\n",
                "        for i in range(first_anchor - 1, -1, -1):\n",
                "            filled[i] = filled[i + 1] + direction\n",
                "    \n",
                "    # Fill trailing gap\n",
                "    if sorted_anchors[-1] < n - 1:\n",
                "        last_anchor = sorted_anchors[-1]\n",
                "        if len(sorted_anchors) >= 2:\n",
                "            direction = coords[last_anchor] - coords[sorted_anchors[-2]]\n",
                "            norm = np.linalg.norm(direction)\n",
                "            direction = direction / norm * avg_distance if norm > 0 else np.array([avg_distance, 0, 0])\n",
                "        else:\n",
                "            direction = np.array([avg_distance, 0, 0])\n",
                "        for i in range(last_anchor + 1, n):\n",
                "            filled[i] = filled[i - 1] + direction\n",
                "    \n",
                "    return filled\n",
                "\n",
                "print(\"Gap filling functions loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. RNA-FM Embeddings (Enhanced)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RNA-FM Encoder loaded\n"
                    ]
                }
            ],
            "source": [
                "class RNAFMEncoder:\n",
                "    \"\"\"RNA-FM embedding encoder with fallback.\"\"\"\n",
                "    \n",
                "    EMBEDDING_DIM = 640\n",
                "    MAX_SEQ_LEN = 500\n",
                "    \n",
                "    def __init__(self, device: str = \"cpu\"):\n",
                "        self.device = device\n",
                "        self.model = None\n",
                "        self._initialized = False\n",
                "        self._try_load_rna_fm()\n",
                "    \n",
                "    def _try_load_rna_fm(self):\n",
                "        \"\"\"Attempt to load RNA-FM model.\"\"\"\n",
                "        try:\n",
                "            import torch\n",
                "            import fm\n",
                "            self.model, self.alphabet = fm.pretrained.rna_fm_t12()\n",
                "            self.model = self.model.to(self.device)\n",
                "            self.model.eval()\n",
                "            self._initialized = True\n",
                "            print(\"RNA-FM model loaded successfully\")\n",
                "        except ImportError:\n",
                "            print(\"RNA-FM not installed, using enhanced fallback embeddings\")\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to load RNA-FM: {e}, using fallback\")\n",
                "    \n",
                "    def encode(self, sequence: str) -> np.ndarray:\n",
                "        \"\"\"Generate embeddings for a sequence.\"\"\"\n",
                "        if self._initialized and len(sequence) <= self.MAX_SEQ_LEN:\n",
                "            return self._encode_with_rna_fm(sequence)\n",
                "        return self._encode_enhanced_fallback(sequence)\n",
                "    \n",
                "    def _encode_with_rna_fm(self, sequence: str) -> np.ndarray:\n",
                "        \"\"\"Encode using RNA-FM model.\"\"\"\n",
                "        import torch\n",
                "        sequence = sequence.upper().replace('T', 'U')\n",
                "        batch_converter = self.alphabet.get_batch_converter()\n",
                "        _, _, batch_tokens = batch_converter([(\"seq\", sequence)])\n",
                "        batch_tokens = batch_tokens.to(self.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            results = self.model(batch_tokens, repr_layers=[12])\n",
                "            embeddings = results[\"representations\"][12]\n",
                "        \n",
                "        return embeddings[0, 1:len(sequence)+1, :].cpu().numpy()\n",
                "    \n",
                "    def _encode_enhanced_fallback(self, sequence: str) -> np.ndarray:\n",
                "        \"\"\"Enhanced fallback with k-mer frequencies and structure propensity.\"\"\"\n",
                "        sequence = sequence.upper().replace('T', 'U')\n",
                "        L = len(sequence)\n",
                "        \n",
                "        # One-hot encoding\n",
                "        nucleotide_map = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
                "        one_hot = np.zeros((L, 4))\n",
                "        for i, nt in enumerate(sequence):\n",
                "            if nt in nucleotide_map:\n",
                "                one_hot[i, nucleotide_map[nt]] = 1.0\n",
                "        \n",
                "        # Positional encoding\n",
                "        pos_dim = self.EMBEDDING_DIM - 4\n",
                "        positions = np.arange(L)[:, np.newaxis]\n",
                "        div_term = np.exp(np.arange(0, pos_dim, 2) * -(np.log(10000.0) / pos_dim))\n",
                "        \n",
                "        pos_encoding = np.zeros((L, pos_dim))\n",
                "        pos_encoding[:, 0::2] = np.sin(positions * div_term[:pos_dim//2])\n",
                "        pos_encoding[:, 1::2] = np.cos(positions * div_term[:pos_dim//2])\n",
                "        \n",
                "        embeddings = np.concatenate([one_hot, pos_encoding], axis=1)\n",
                "        embeddings = embeddings / (np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8)\n",
                "        \n",
                "        return embeddings.astype(np.float32)\n",
                "    \n",
                "    @property\n",
                "    def is_rna_fm_available(self) -> bool:\n",
                "        return self._initialized\n",
                "\n",
                "print(\"RNA-FM Encoder loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. TM-Score Computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TM-score computation loaded\n"
                    ]
                }
            ],
            "source": [
                "def compute_tm_score(pred_coords: np.ndarray, ref_coords: np.ndarray) -> float:\n",
                "    \"\"\"Compute TM-score between predicted and reference coordinates.\"\"\"\n",
                "    L = len(pred_coords)\n",
                "    \n",
                "    if L == 0 or len(ref_coords) != L:\n",
                "        return 0.0\n",
                "    \n",
                "    valid_mask = ~(np.isnan(pred_coords).any(axis=1) | np.isnan(ref_coords).any(axis=1))\n",
                "    if np.sum(valid_mask) < 3:\n",
                "        return 0.0\n",
                "    \n",
                "    pred = pred_coords[valid_mask]\n",
                "    ref = ref_coords[valid_mask]\n",
                "    L_valid = len(pred)\n",
                "    \n",
                "    d0 = max(0.5, 1.24 * (L_valid - 15) ** (1/3) - 1.8)\n",
                "    \n",
                "    # Center structures\n",
                "    pred_centered = pred - pred.mean(axis=0)\n",
                "    ref_centered = ref - ref.mean(axis=0)\n",
                "    \n",
                "    # Kabsch algorithm for optimal rotation\n",
                "    try:\n",
                "        H = pred_centered.T @ ref_centered\n",
                "        U, S, Vt = np.linalg.svd(H)\n",
                "        R = Vt.T @ U.T\n",
                "        if np.linalg.det(R) < 0:\n",
                "            Vt[-1, :] *= -1\n",
                "            R = Vt.T @ U.T\n",
                "        pred_rotated = pred_centered @ R\n",
                "    except np.linalg.LinAlgError:\n",
                "        pred_rotated = pred_centered\n",
                "    \n",
                "    distances = np.sqrt(np.sum((pred_rotated - ref_centered) ** 2, axis=1))\n",
                "    tm_score = np.sum(1.0 / (1.0 + (distances / d0) ** 2)) / L_valid\n",
                "    \n",
                "    return tm_score\n",
                "\n",
                "print(\"TM-score computation loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Submission Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Submission functions loaded\n"
                    ]
                }
            ],
            "source": [
                "def diversify_models(base_coords: np.ndarray, num_models: int = 5, scale: float = 0.2) -> List[np.ndarray]:\n",
                "    \"\"\"Create diverse models from base prediction.\"\"\"\n",
                "    models = [base_coords.copy()]\n",
                "    for i in range(1, num_models):\n",
                "        perturbed = base_coords + np.random.randn(*base_coords.shape) * (scale * i)\n",
                "        models.append(np.clip(perturbed, -999.999, 9999.999))\n",
                "    return models\n",
                "\n",
                "\n",
                "def create_submission_df(predictions: List[dict]) -> pd.DataFrame:\n",
                "    \"\"\"Create submission DataFrame from predictions.\"\"\"\n",
                "    rows = []\n",
                "    \n",
                "    for pred in predictions:\n",
                "        target_id = pred['target_id']\n",
                "        sequence = pred['sequence']\n",
                "        models = pred['models']\n",
                "        \n",
                "        for i, base in enumerate(sequence):\n",
                "            resid = i + 1\n",
                "            row = {'ID': f\"{target_id}_{resid}\", 'resname': base, 'resid': resid}\n",
                "            \n",
                "            for model_idx in range(5):\n",
                "                coords = np.clip(models[model_idx][i], -999.999, 9999.999)\n",
                "                row[f'x_{model_idx + 1}'] = round(float(coords[0]), 3)\n",
                "                row[f'y_{model_idx + 1}'] = round(float(coords[1]), 3)\n",
                "                row[f'z_{model_idx + 1}'] = round(float(coords[2]), 3)\n",
                "            \n",
                "            rows.append(row)\n",
                "    \n",
                "    columns = ['ID', 'resname', 'resid']\n",
                "    for i in range(1, 6):\n",
                "        columns.extend([f'x_{i}', f'y_{i}', f'z_{i}'])\n",
                "    \n",
                "    return pd.DataFrame(rows, columns=columns)\n",
                "\n",
                "print(\"Submission functions loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Main TBM Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Pipeline function loaded\n"
                    ]
                }
            ],
            "source": [
                "def run_tbm_pipeline(test_sequences_file: str = None, use_cache: bool = True) -> pd.DataFrame:\n",
                "    \"\"\"Main TBM pipeline execution.\"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Set up paths\n",
                "    pdb_rna_dir = DATA_DIR / \"PDB_RNA\"\n",
                "    release_dates_file = DATA_DIR / \"extra\" / \"rna_metadata.csv\"\n",
                "    if not release_dates_file.exists():\n",
                "        release_dates_file = DATA_DIR / \"rna_metadata.csv\"\n",
                "    \n",
                "    if test_sequences_file is None:\n",
                "        test_sequences_file = DATA_DIR / \"data\" / \"sequences\" / \"test_sequences.csv\"\n",
                "    template_db_path = OUTPUT_DIR / \"template_db.pkl\"\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    print(\"Stanford RNA 3D Folding - TBM Pipeline\")\n",
                "    print(\"=\" * 60)\n",
                "    print(f\"PDB directory: {pdb_rna_dir}\")\n",
                "    print(f\"Release dates: {release_dates_file}\")\n",
                "    print(f\"Test sequences: {test_sequences_file}\")\n",
                "    \n",
                "    # Load or build template database\n",
                "    if use_cache and template_db_path.exists():\n",
                "        print(f\"\\nLoading cached template database...\")\n",
                "        template_db = TemplateDB.load(str(template_db_path))\n",
                "    else:\n",
                "        print(\"\\nBuilding template database...\")\n",
                "        template_db = TemplateDB(k=config.kmer_size)\n",
                "        template_db.build_from_directory(\n",
                "            str(pdb_rna_dir),\n",
                "            str(release_dates_file) if release_dates_file.exists() else None,\n",
                "            max_files=config.max_files\n",
                "        )\n",
                "        template_db.save(str(template_db_path))\n",
                "    \n",
                "    db_time = time.time() - start_time\n",
                "    print(f\"Template database ready in {db_time:.1f}s ({len(template_db.templates)} templates)\")\n",
                "    \n",
                "    # Load test sequences\n",
                "    test_df = pd.read_csv(test_sequences_file)\n",
                "    print(f\"\\nLoaded {len(test_df)} test targets\")\n",
                "    \n",
                "    # Process each target\n",
                "    predictions = []\n",
                "    num_with_templates = 0\n",
                "    num_no_templates = 0\n",
                "    \n",
                "    for idx, row in test_df.iterrows():\n",
                "        target_id = row['target_id']\n",
                "        sequence = row['sequence']\n",
                "        temporal_cutoff = row.get('temporal_cutoff', '2099-12-31')\n",
                "        \n",
                "        hits = template_db.search(sequence, temporal_cutoff, max_hits=config.max_template_hits)\n",
                "        models = []\n",
                "        \n",
                "        if hits:\n",
                "            num_with_templates += 1\n",
                "            best_hit = hits[0]\n",
                "            print(f\"[{idx+1}/{len(test_df)}] {target_id} ({len(sequence)}nt): \"\n",
                "                  f\"Best: {best_hit.pdb_id}_{best_hit.chain_id} (id={best_hit.identity:.2f})\")\n",
                "            \n",
                "            for hit in hits[:config.num_models]:\n",
                "                template_coords = template_db.get_template_coords(hit.pdb_id, hit.chain_id)\n",
                "                if template_coords:\n",
                "                    coords = transfer_coordinates(sequence, template_coords)\n",
                "                    coords = fill_gaps(coords, config.average_c1_distance)\n",
                "                    models.append(coords)\n",
                "            \n",
                "            while len(models) < config.num_models:\n",
                "                if models:\n",
                "                    perturbed = models[0] + np.random.randn(len(sequence), 3) * config.perturbation_scale * len(models)\n",
                "                    models.append(np.clip(perturbed, -999.999, 9999.999))\n",
                "                else:\n",
                "                    models.append(generate_geometric_baseline(len(sequence)))\n",
                "        else:\n",
                "            num_no_templates += 1\n",
                "            print(f\"[{idx+1}/{len(test_df)}] {target_id} ({len(sequence)}nt): No templates\")\n",
                "            base = generate_geometric_baseline(len(sequence))\n",
                "            models = diversify_models(base, config.num_models)\n",
                "        \n",
                "        predictions.append({\n",
                "            'target_id': target_id,\n",
                "            'sequence': sequence,\n",
                "            'models': models[:config.num_models]\n",
                "        })\n",
                "    \n",
                "    # Create submission\n",
                "    print(\"\\nGenerating submission...\")\n",
                "    submission_df = create_submission_df(predictions)\n",
                "    submission_path = OUTPUT_DIR / \"submission.csv\"\n",
                "    submission_df.to_csv(submission_path, index=False)\n",
                "    \n",
                "    total_time = time.time() - start_time\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"PIPELINE COMPLETE\")\n",
                "    print(\"=\" * 60)\n",
                "    print(f\"Targets: {len(test_df)} (with templates: {num_with_templates}, none: {num_no_templates})\")\n",
                "    print(f\"Rows: {len(submission_df)}\")\n",
                "    print(f\"Time: {total_time:.1f}s\")\n",
                "    print(f\"Output: {submission_path}\")\n",
                "    \n",
                "    return submission_df\n",
                "\n",
                "print(\"Pipeline function loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Run Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "Stanford RNA 3D Folding - TBM Pipeline\n",
                        "============================================================\n",
                        "PDB directory: /Users/taher/Projects/stanford-rna-3d-folding-2/PDB_RNA\n",
                        "Release dates: /Users/taher/Projects/stanford-rna-3d-folding-2/extra/rna_metadata.csv\n",
                        "Test sequences: /Users/taher/Projects/stanford-rna-3d-folding-2/data/sequences/test_sequences.csv\n",
                        "\n",
                        "Loading cached template database...\n",
                        "Template database ready in 1.7s (1982 templates)\n",
                        "\n",
                        "Loaded 28 test targets\n",
                        "[1/28] 8ZNQ (30nt): Best: 8B2L_A3 (id=0.50)\n",
                        "[2/28] 9IWF (69nt): Best: 5ZET_A (id=0.41)\n",
                        "[3/28] 9JGM (210nt): Best: 8HKY_A23S (id=0.34)\n",
                        "[4/28] 9MME (4640nt): No templates\n",
                        "[5/28] 9J09 (214nt): Best: 6XYW_1 (id=0.34)\n",
                        "[6/28] 9E9Q (101nt): Best: 6CAE_1A (id=0.37)\n",
                        "[7/28] 9CFN (59nt): Best: 5LZZ_5 (id=0.46)\n",
                        "[8/28] 9OBM (73nt): Best: 5MRF_A (id=0.41)\n",
                        "[9/28] 9G4P (68nt): Best: 5D8B_VC (id=0.49)\n",
                        "[10/28] 9G4Q (104nt): Best: 4V7H_B5 (id=0.39)\n",
                        "[11/28] 9G4R (47nt): Best: 6HD7_1 (id=0.45)\n",
                        "[12/28] 9RVP (34nt): Best: 6ZU5_L50 (id=0.47)\n",
                        "[13/28] 9JFS (246nt): Best: 7ZJX_L5 (id=0.32)\n",
                        "[14/28] 9LEC (378nt): Best: 9LMF_A (id=0.73)\n",
                        "[15/28] 9LEL (476nt): Best: 6HRM_1 (id=0.31)\n",
                        "[16/28] 9I9W (28nt): Best: 6HRM_1 (id=0.57)\n",
                        "[17/28] 9HRO (35nt): Best: 6HRM_1 (id=0.51)\n",
                        "[18/28] 9QZJ (19nt): Best: 6O8W_A (id=0.68)\n",
                        "[19/28] 9JFO (195nt): Best: 7KGB_A (id=0.34)\n",
                        "[20/28] 9OD4 (23nt): Best: 5UMD_A (id=0.65)\n",
                        "[21/28] 9WHV (80nt): Best: 8A5I_A (id=0.33)\n",
                        "[22/28] 9E74 (255nt): Best: 6ZME_L5 (id=0.33)\n",
                        "[23/28] 9E75 (165nt): Best: 8A5I_A (id=0.35)\n",
                        "[24/28] 9G4J (334nt): Best: 4DS6_A (id=0.99)\n",
                        "[25/28] 9KGG (267nt): Best: 9G1Z_1 (id=0.36)\n",
                        "[26/28] 9EBP (81nt): Best: 6SPG_A (id=0.41)\n",
                        "[27/28] 9LJN (71nt): Best: 7KGB_A (id=0.45)\n",
                        "[28/28] 9ZCC (1460nt): Best: 6Y0G_L5 (id=0.29)\n",
                        "\n",
                        "Generating submission...\n",
                        "\n",
                        "============================================================\n",
                        "PIPELINE COMPLETE\n",
                        "============================================================\n",
                        "Targets: 28 (with templates: 27, none: 1)\n",
                        "Rows: 9762\n",
                        "Time: 171.5s\n",
                        "Output: /Users/taher/Projects/stanford-rna-3d-folding-2/output/submission.csv\n",
                        "\n",
                        "Submission shape: (9762, 18)\n",
                        "\n",
                        "First few rows:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>ID</th>\n",
                            "      <th>resname</th>\n",
                            "      <th>resid</th>\n",
                            "      <th>x_1</th>\n",
                            "      <th>y_1</th>\n",
                            "      <th>z_1</th>\n",
                            "      <th>x_2</th>\n",
                            "      <th>y_2</th>\n",
                            "      <th>z_2</th>\n",
                            "      <th>x_3</th>\n",
                            "      <th>y_3</th>\n",
                            "      <th>z_3</th>\n",
                            "      <th>x_4</th>\n",
                            "      <th>y_4</th>\n",
                            "      <th>z_4</th>\n",
                            "      <th>x_5</th>\n",
                            "      <th>y_5</th>\n",
                            "      <th>z_5</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>8ZNQ_1</td>\n",
                            "      <td>A</td>\n",
                            "      <td>1</td>\n",
                            "      <td>186.567</td>\n",
                            "      <td>265.833</td>\n",
                            "      <td>250.541</td>\n",
                            "      <td>59.879</td>\n",
                            "      <td>16.731</td>\n",
                            "      <td>-43.706</td>\n",
                            "      <td>219.287</td>\n",
                            "      <td>131.017</td>\n",
                            "      <td>244.078</td>\n",
                            "      <td>133.110</td>\n",
                            "      <td>147.774</td>\n",
                            "      <td>171.323</td>\n",
                            "      <td>102.186</td>\n",
                            "      <td>2.942</td>\n",
                            "      <td>14.346</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>8ZNQ_2</td>\n",
                            "      <td>C</td>\n",
                            "      <td>2</td>\n",
                            "      <td>184.576</td>\n",
                            "      <td>276.194</td>\n",
                            "      <td>251.236</td>\n",
                            "      <td>60.613</td>\n",
                            "      <td>21.832</td>\n",
                            "      <td>-45.707</td>\n",
                            "      <td>216.234</td>\n",
                            "      <td>131.217</td>\n",
                            "      <td>257.541</td>\n",
                            "      <td>144.311</td>\n",
                            "      <td>125.928</td>\n",
                            "      <td>178.112</td>\n",
                            "      <td>90.463</td>\n",
                            "      <td>5.124</td>\n",
                            "      <td>29.777</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>8ZNQ_3</td>\n",
                            "      <td>C</td>\n",
                            "      <td>3</td>\n",
                            "      <td>194.833</td>\n",
                            "      <td>281.150</td>\n",
                            "      <td>262.882</td>\n",
                            "      <td>68.972</td>\n",
                            "      <td>34.928</td>\n",
                            "      <td>-34.925</td>\n",
                            "      <td>219.474</td>\n",
                            "      <td>133.402</td>\n",
                            "      <td>261.433</td>\n",
                            "      <td>139.565</td>\n",
                            "      <td>125.552</td>\n",
                            "      <td>166.683</td>\n",
                            "      <td>74.881</td>\n",
                            "      <td>-15.045</td>\n",
                            "      <td>29.030</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>8ZNQ_4</td>\n",
                            "      <td>G</td>\n",
                            "      <td>4</td>\n",
                            "      <td>211.244</td>\n",
                            "      <td>265.903</td>\n",
                            "      <td>244.790</td>\n",
                            "      <td>63.215</td>\n",
                            "      <td>56.398</td>\n",
                            "      <td>-36.282</td>\n",
                            "      <td>209.438</td>\n",
                            "      <td>157.398</td>\n",
                            "      <td>263.353</td>\n",
                            "      <td>147.881</td>\n",
                            "      <td>128.861</td>\n",
                            "      <td>160.490</td>\n",
                            "      <td>66.363</td>\n",
                            "      <td>-7.155</td>\n",
                            "      <td>39.541</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>8ZNQ_5</td>\n",
                            "      <td>U</td>\n",
                            "      <td>5</td>\n",
                            "      <td>224.713</td>\n",
                            "      <td>250.801</td>\n",
                            "      <td>227.073</td>\n",
                            "      <td>48.675</td>\n",
                            "      <td>53.568</td>\n",
                            "      <td>-20.729</td>\n",
                            "      <td>207.388</td>\n",
                            "      <td>155.650</td>\n",
                            "      <td>269.402</td>\n",
                            "      <td>159.585</td>\n",
                            "      <td>129.415</td>\n",
                            "      <td>162.282</td>\n",
                            "      <td>48.807</td>\n",
                            "      <td>-10.971</td>\n",
                            "      <td>31.850</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>8ZNQ_6</td>\n",
                            "      <td>G</td>\n",
                            "      <td>6</td>\n",
                            "      <td>215.867</td>\n",
                            "      <td>241.832</td>\n",
                            "      <td>223.686</td>\n",
                            "      <td>49.028</td>\n",
                            "      <td>59.201</td>\n",
                            "      <td>-21.389</td>\n",
                            "      <td>221.908</td>\n",
                            "      <td>152.680</td>\n",
                            "      <td>290.588</td>\n",
                            "      <td>157.059</td>\n",
                            "      <td>118.148</td>\n",
                            "      <td>172.992</td>\n",
                            "      <td>46.049</td>\n",
                            "      <td>-23.273</td>\n",
                            "      <td>47.742</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>8ZNQ_7</td>\n",
                            "      <td>A</td>\n",
                            "      <td>7</td>\n",
                            "      <td>210.311</td>\n",
                            "      <td>239.782</td>\n",
                            "      <td>218.445</td>\n",
                            "      <td>60.513</td>\n",
                            "      <td>61.935</td>\n",
                            "      <td>-9.692</td>\n",
                            "      <td>217.846</td>\n",
                            "      <td>154.515</td>\n",
                            "      <td>295.324</td>\n",
                            "      <td>161.447</td>\n",
                            "      <td>127.977</td>\n",
                            "      <td>172.873</td>\n",
                            "      <td>45.533</td>\n",
                            "      <td>-19.213</td>\n",
                            "      <td>58.293</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>8ZNQ_8</td>\n",
                            "      <td>C</td>\n",
                            "      <td>8</td>\n",
                            "      <td>207.860</td>\n",
                            "      <td>244.608</td>\n",
                            "      <td>220.111</td>\n",
                            "      <td>62.540</td>\n",
                            "      <td>71.626</td>\n",
                            "      <td>-10.844</td>\n",
                            "      <td>213.700</td>\n",
                            "      <td>151.742</td>\n",
                            "      <td>299.442</td>\n",
                            "      <td>104.359</td>\n",
                            "      <td>192.955</td>\n",
                            "      <td>199.600</td>\n",
                            "      <td>24.579</td>\n",
                            "      <td>-27.276</td>\n",
                            "      <td>55.429</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>8ZNQ_9</td>\n",
                            "      <td>G</td>\n",
                            "      <td>9</td>\n",
                            "      <td>207.234</td>\n",
                            "      <td>246.135</td>\n",
                            "      <td>225.151</td>\n",
                            "      <td>42.025</td>\n",
                            "      <td>79.354</td>\n",
                            "      <td>-0.660</td>\n",
                            "      <td>208.054</td>\n",
                            "      <td>183.923</td>\n",
                            "      <td>294.041</td>\n",
                            "      <td>99.258</td>\n",
                            "      <td>193.520</td>\n",
                            "      <td>201.351</td>\n",
                            "      <td>24.595</td>\n",
                            "      <td>-29.239</td>\n",
                            "      <td>60.437</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>8ZNQ_10</td>\n",
                            "      <td>G</td>\n",
                            "      <td>10</td>\n",
                            "      <td>232.802</td>\n",
                            "      <td>262.289</td>\n",
                            "      <td>232.712</td>\n",
                            "      <td>42.405</td>\n",
                            "      <td>76.652</td>\n",
                            "      <td>10.031</td>\n",
                            "      <td>211.515</td>\n",
                            "      <td>183.488</td>\n",
                            "      <td>289.908</td>\n",
                            "      <td>93.166</td>\n",
                            "      <td>199.121</td>\n",
                            "      <td>212.339</td>\n",
                            "      <td>22.042</td>\n",
                            "      <td>-34.455</td>\n",
                            "      <td>57.330</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "        ID resname  resid      x_1      y_1      z_1     x_2     y_2     z_2  \\\n",
                            "0   8ZNQ_1       A      1  186.567  265.833  250.541  59.879  16.731 -43.706   \n",
                            "1   8ZNQ_2       C      2  184.576  276.194  251.236  60.613  21.832 -45.707   \n",
                            "2   8ZNQ_3       C      3  194.833  281.150  262.882  68.972  34.928 -34.925   \n",
                            "3   8ZNQ_4       G      4  211.244  265.903  244.790  63.215  56.398 -36.282   \n",
                            "4   8ZNQ_5       U      5  224.713  250.801  227.073  48.675  53.568 -20.729   \n",
                            "5   8ZNQ_6       G      6  215.867  241.832  223.686  49.028  59.201 -21.389   \n",
                            "6   8ZNQ_7       A      7  210.311  239.782  218.445  60.513  61.935  -9.692   \n",
                            "7   8ZNQ_8       C      8  207.860  244.608  220.111  62.540  71.626 -10.844   \n",
                            "8   8ZNQ_9       G      9  207.234  246.135  225.151  42.025  79.354  -0.660   \n",
                            "9  8ZNQ_10       G     10  232.802  262.289  232.712  42.405  76.652  10.031   \n",
                            "\n",
                            "       x_3      y_3      z_3      x_4      y_4      z_4      x_5     y_5  \\\n",
                            "0  219.287  131.017  244.078  133.110  147.774  171.323  102.186   2.942   \n",
                            "1  216.234  131.217  257.541  144.311  125.928  178.112   90.463   5.124   \n",
                            "2  219.474  133.402  261.433  139.565  125.552  166.683   74.881 -15.045   \n",
                            "3  209.438  157.398  263.353  147.881  128.861  160.490   66.363  -7.155   \n",
                            "4  207.388  155.650  269.402  159.585  129.415  162.282   48.807 -10.971   \n",
                            "5  221.908  152.680  290.588  157.059  118.148  172.992   46.049 -23.273   \n",
                            "6  217.846  154.515  295.324  161.447  127.977  172.873   45.533 -19.213   \n",
                            "7  213.700  151.742  299.442  104.359  192.955  199.600   24.579 -27.276   \n",
                            "8  208.054  183.923  294.041   99.258  193.520  201.351   24.595 -29.239   \n",
                            "9  211.515  183.488  289.908   93.166  199.121  212.339   22.042 -34.455   \n",
                            "\n",
                            "      z_5  \n",
                            "0  14.346  \n",
                            "1  29.777  \n",
                            "2  29.030  \n",
                            "3  39.541  \n",
                            "4  31.850  \n",
                            "5  47.742  \n",
                            "6  58.293  \n",
                            "7  55.429  \n",
                            "8  60.437  \n",
                            "9  57.330  "
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Run the pipeline\n",
                "submission = run_tbm_pipeline()\n",
                "\n",
                "print(f\"\\nSubmission shape: {submission.shape}\")\n",
                "print(\"\\nFirst few rows:\")\n",
                "submission.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Validation (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Validation labels found, loading...\n",
                        "Loaded 28 ground truth structures\n"
                    ]
                }
            ],
            "source": [
                "def load_ground_truth(labels_file: str) -> dict:\n",
                "    \"\"\"Load ground truth coordinates from labels file.\"\"\"\n",
                "    df = pd.read_csv(labels_file)\n",
                "    ground_truth = {}\n",
                "    \n",
                "    for _, row in df.iterrows():\n",
                "        id_str = row['ID']\n",
                "        parts = id_str.rsplit('_', 1)\n",
                "        target_id = parts[0]\n",
                "        resid = int(parts[1])\n",
                "        \n",
                "        if target_id not in ground_truth:\n",
                "            ground_truth[target_id] = {}\n",
                "        \n",
                "        x, y, z = row['x_1'], row['y_1'], row['z_1']\n",
                "        if x < -1e10:\n",
                "            continue\n",
                "        ground_truth[target_id][resid] = np.array([x, y, z])\n",
                "    \n",
                "    result = {}\n",
                "    for target_id, residues in ground_truth.items():\n",
                "        if not residues:\n",
                "            continue\n",
                "        max_resid = max(residues.keys())\n",
                "        coords = np.full((max_resid, 3), np.nan)\n",
                "        for resid, coord in residues.items():\n",
                "            coords[resid - 1] = coord\n",
                "        result[target_id] = coords\n",
                "    \n",
                "    return result\n",
                "\n",
                "# Check for validation data\n",
                "val_labels = DATA_DIR / \"data\" / \"validation_labels.csv\"\n",
                "if val_labels.exists():\n",
                "    print(\"Validation labels found, loading...\")\n",
                "    ground_truth = load_ground_truth(str(val_labels))\n",
                "    print(f\"Loaded {len(ground_truth)} ground truth structures\")\n",
                "else:\n",
                "    print(\"No validation labels found (expected for Kaggle test set)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "SUBMISSION STATISTICS\n",
                        "============================================================\n",
                        "Total rows: 9762\n",
                        "Unique targets: 28\n",
                        "\n",
                        "Coordinate ranges:\n",
                        "  x_1: [-84.69, 389.74]\n",
                        "  y_1: [-120.03, 356.65]\n",
                        "  z_1: [-116.76, 10000.00]\n",
                        "\n",
                        "Memory usage: 2.19 MB\n"
                    ]
                }
            ],
            "source": [
                "# Summary statistics\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"SUBMISSION STATISTICS\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Total rows: {len(submission)}\")\n",
                "print(f\"Unique targets: {submission['ID'].str.rsplit('_', n=1).str[0].nunique()}\")\n",
                "print(f\"\\nCoordinate ranges:\")\n",
                "for col in ['x_1', 'y_1', 'z_1']:\n",
                "    print(f\"  {col}: [{submission[col].min():.2f}, {submission[col].max():.2f}]\")\n",
                "print(f\"\\nMemory usage: {submission.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Compute TM-scores: evaluate predictions against ground truth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "object of type 'ellipsis' has no len()",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m target_id, gt_coords \u001b[38;5;129;01min\u001b[39;00m ground_truth.items():\n\u001b[32m      2\u001b[39m     pred_coords = ...  \u001b[38;5;66;03m# extract from submission\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tm = \u001b[43mcompute_tm_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_coords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: TM-score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtm\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcompute_tm_score\u001b[39m\u001b[34m(pred_coords, ref_coords)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_tm_score\u001b[39m(pred_coords: np.ndarray, ref_coords: np.ndarray) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute TM-score between predicted and reference coordinates.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     L = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred_coords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m L == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ref_coords) != L:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n",
                        "\u001b[31mTypeError\u001b[39m: object of type 'ellipsis' has no len()"
                    ]
                }
            ],
            "source": [
                "for target_id, gt_coords in ground_truth.items():\n",
                "    pred_coords = ...  # extract from submission\n",
                "    tm = compute_tm_score(pred_coords, gt_coords)\n",
                "    print(f\"{target_id}: TM-score = {tm:.3f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (RNA-TBM)",
            "language": "python",
            "name": "rna-tbm"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
